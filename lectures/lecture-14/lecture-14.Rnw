\documentclass{beamer}
\usepackage{../371g-slides}
\title{Model Building 1}
\subtitle{Lecture 14}
\author{STA 371G}

\begin{document}
  <<setup, include=F, cache=F>>=
  opts_knit$set(global.par=T)
  knit_hooks$set(crop=hook_pdfcrop)
  opts_chunk$set(dev='tikz', external=F, fig.path='/tmp/figures/', comment=NA, fig.width=4, fig.height=3, crop=T, sanitize=T, tidy=F)
  knit_theme$set('camo')
  @
  <<include=F, cache=F>>=
  library(car)
  library(stats)
  library(leaps)
  counties <- read.csv("../../data/counties.csv", na.strings="")
  par(fg='#fefefe', col.axis='#fefefe', col.lab='#fefefe', col.main="#fefefe", mar=c(5.1, 4.1, 1.1, 2.1))
  @

  \frame{\maketitle}

  % Show outline at beginning of each section
  \AtBeginSection[]{ 
    \begin{frame}<beamer>
      \tableofcontents[currentsection]
    \end{frame}
  }

  %%%%%%% Slides start here %%%%%%%

  \begin{darkframes}    
    \begin{frame}
      \fullpagepicture{DocShortage}
    \end{frame}

    \begin{frame}{What might explain this?}
      \begin{columns}[onlytextwidth]
        \column{.5\textwidth} 
          \begin{itemize}
            \item Small counties
            \item Poverty
            \item Health insurance
          \end{itemize}
        \column{.5\textwidth}
          \begin{itemize}
            \item Unemployment
            \item Large rural areas
            \item Something else?
          \end{itemize}
      \end{columns}
      
      \lc %{smallest population}
    \end{frame}

    \begin{frame}[fragile]{What to do if there a lot of potential predictors}
      \begin{itemize}[<+->]
        \item Previously, we assumed that the explanatory variables were either from a small set or chosen in advance.
        \item However, figuring out what variables to use to predict the number of physicians that a county has, is a critical portion of the analysis in this case.
        \item This type of analysis is an \textbf{exploratory study}.
      \end{itemize} 
    \end{frame}

    \begin{frame}[fragile]{An exploratory study of the Texas physician shortage}
      \begin{itemize}[<+->]
        \item Exploratory studies are observational studies, in that the variables are observed rather than controlled.
        \item Multicollinearity is much more likely in an exploratory study than in an experiment or a confirmatory study.
        \item Exploratory studies require the most in terms of model selection. Automated tools are helpful, but judgement is still needed!
      \end{itemize} 
    \end{frame}

    \begin{frame}[fragile]{Population as a predictor of number of physicians}
      <<fig.height=2>>=
      plot(counties$Population, counties$Physicians)
      popmodel <- lm(counties$Physicians ~ counties$Population)
      abline(popmodel)
      @

      \lc %{What is R2 if we predict physicians from population?}
    \end{frame}

    \begin{frame}[fragile]{Transform and Subset the data}
      <<>>=

      # Create a variable for physicians per 10,000 people
      counties$PhysiciansPer10000 <-
        counties$Physicians / counties$Population * 10000

      # Remove the very small and very large counties
      mcounties <- counties[counties$Population < 500000 & 
                            counties$Population > 10000,]

      # Which medium counties have no physicians?
      mcounties[mcounties$Physicians == 0, c(1,5,12)]
      @

      \lc %{Why would we want to remove}
    \end{frame}

    \begin{frame}[fragile]{Potential predictor variables}
      \begin{itemize}
        \item \textbf{LandArea}:       Area in square miles
        \item \textbf{PctRural}:       Percentage rural land
        \item \textbf{MedianIncome}:   Median household income
        \item \textbf{Population}:     Population
        \item \textbf{PctUnder18}:     Percent children
        \item \textbf{PctOver65}:      Percent seniors
        \item \textbf{PctPoverty}:     Percent below the poverty line
        \item \textbf{PctUninsured}:   Percent without health insurance
        \item \textbf{PctSomeCollege}: Percent with some higher education
        \item \textbf{PctUnemployed}:  Percent unemployed
      \end{itemize}
    \end{frame}

    \begin{frame}{Building all of the possible models}
      \begin{itemize}[<+->]
        \item Previously, we built the full model and eliminated the variables in order of largest 
        $p$-value (or smallest $t$-score). This is called \textbf{backward stepwise regression}.
        \item This method is not guaranteed to find to the best model!
        \item If there are $n$ candidate predictor variables, there are $2^n-1$ possible models, and we would need to look at every one of them to be sure that we have found the best model.
        \item This is where R's automated model building tools help.
      \end{itemize} 
      \lc  %{If there are 5 candidate predictor variables}
    \end{frame}

    \begin{frame}{How to decide which model is best}
      \begin{itemize}[<+->]
        \item We have used $R^2$ and Adjusted-$R^2$ to select the best models
        \item But $R^2$ is not good for comparing models with different numbers of variables because it tends to increase a little bit with each additional variable, just due to randomness.
        \item Adjusted-$R^2$ is better because it multiplies $R^2$ by a penalty that depends on the number of variables, but the penalty is somewhat arbitrary and increases as the number of variables increases.
      \end{itemize} 
    \end{frame}


    \begin{frame}{There are many ways to decide which model is best}
      \begin{itemize}[<+->]
        \item All model selection criteria try to find a balance between the \alert{predictive power of the model} and the \alert{complexity of the model} (number of variables).
        \item No method is ideal in all situations, so it is generally best to use multiple methods and look at the results.
        \item AIC (Akaike's Information Criterion) and the very similar BIC (the reading calls it SBC) are other widely used criterion that have a similar intent as Adjusted-$R^2$ but may give different results.
        \item There are other selection criteria too (but we won't get into them in this course).
      \end{itemize} 
    \end{frame}

    \begin{frame}[fragile]{Stepping forwards}
       The \texttt{step} function uses the AIC criterion to compare models. First we'll build a ``null model'' with no variables, and a ``full model'' with all variables:

      \fontsize{9}{9}\selectfont
      <<results='hide'>>=
      null <- lm(PhysiciansPer10000 ~ 1, data=mcounties)

      full <- lm(PhysiciansPer10000 ~ LandArea + PctRural 
                  + MedianIncome + Population + PctUnder18 
                  + PctOver65 + PctPoverty + PctUninsured 
                  + PctSomeCollege + PctUnemployed, 
                  data=mcounties)

      forward.model <- step(null, 
                            scope=list(lower=null, upper=full), 
                            direction="forward")
      @

      \lc %{Examine stepforwardOut. Which variable is the most significant}
    \end{frame}

  
    \begin{frame}[fragile]{Stepping backwards and both ways}
      You can also step backwards (similar to what we have been doing manually), 
      or in both directions:

      \fontsize{9}{9}\selectfont
      <<results='hide'>>=    
      backward.model <- step(full, 
                             scope=list(lower=null, upper=full), 
                             direction="backward")

      both.model <- step(null, 
                         scope=list(lower=null, upper=full), 
                         direction="both")
      @

      \lc
    \end{frame}


    \begin{frame}[fragile]{Check assumptions}
      <<echo=F>>=
      par(mfrow=c(2,2)) # change the panel layout to 2x2
      @
      <<fig.width=4.5>>=
      plot(backward.model)
      @
      <<echo=F>>=
      par(mfrow=c(1,1)) # change the panel layout back
      @
    \end{frame}


    \begin{frame}[fragile]{Check for multicollinearity}
      \fontsize{8}{8}\selectfont
      <<>>=
      vif(backward.model)
      @
    \end{frame}

    \begin{frame}
      \begin{center}
        We can't be sure this is the best possible model. 

        \bigskip

        Sometimes, stepwise regression leads you down a suboptimal path and you end up discarding a valuable variable (or keeping a variable that is only marginally useful), because of the order in which the variables are considered.
      \end{center}
    \end{frame}

    \begin{frame}{Best-subsets regression}
      \begin{itemize}[<+->]
        \item \textbf{Best-subsets regression} compares every possible model containing some subset of the predictor variables!
        \item Then we can compare the models using different model selection criteria and select the most parsimonious one
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Best-subsets regression}
     <<>>=
     regsubsets.output <- 
       regsubsets(PhysiciansPer10000 ~ LandArea + PctRural 
                  + MedianIncome + Population + PctUnder18 
                  + PctOver65 + PctPoverty + PctUninsured
                  + PctSomeCollege + PctUnemployed, 
                  data=mcounties)
     @
    \end{frame}




    \begin{frame}[fragile]
      \fontsize{9}{9}\selectfont
      Let's compare models using Adjusted $R^2$. Each row is a candidate model; filled-in squares indicate the variable is included in that model:

      <<fig.keep='none'>>=
      plot(regsubsets.output, scale="adjr2")
      @

      \vspace{-1in}

      <<echo=F, fig.height=4>>=
      plot(regsubsets.output, scale="adjr2", col='orange')
      @
    \end{frame}

    \begin{frame}[fragile]
      \fontsize{9}{9}\selectfont
      Now let's compare models using BIC (SBC):

      <<fig.keep='none'>>=
      plot(regsubsets.output, scale="bic")
      @

      \vspace{-1in}

      <<echo=F, fig.height=4>>=
      plot(regsubsets.output, scale="bic", col='orange')
      @
    \end{frame}

    \begin{frame}{Putting things together}
      \begin{itemize}[<+->]
        \item Look at multiple statistics. They generally say similar things.
        \item Find the \textbf{parsimonious} middle ground between an underspecified model and extraneous variables.
        \item Fine-tune the model to ensure the model meets assumptions and captures key relationships: you may need to transform predictors and/or add interactions.
        \item Think about logical reasons why certain predictors might be useful, don't just focus on $p$-values.
      \end{itemize} 
    \end{frame}


    \begin{frame}{Be careful of getting too crazy!}
      \begin{itemize}[<+->]
        \item A general guideline is that you should not even consider more than one variable for every 10 to 15 cases in your dataset. 
        \item Otherwise, you can select the ones that happen to fit the data the best and essentially create a spurious correlation!
        \item Remember to check for multicolliearity and the LINE assumptions!
      \end{itemize} 
    \end{frame}
  \end{darkframes}
\end{document}