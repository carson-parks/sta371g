\documentclass{beamer}
\usepackage{../371g-slides}
\title{Model Building 2}
\subtitle{Lecture 15}
\author{STA 371G}

\begin{document}
  <<setup, include=F, cache=F>>=
  opts_knit$set(global.par=T)
  knit_hooks$set(crop=hook_pdfcrop)
  opts_chunk$set(dev='tikz', external=F, fig.path='/tmp/figures/', comment=NA, fig.width=4, fig.height=3, crop=T, sanitize=T, tidy=F)
  knit_theme$set('camo')
  @
  <<include=F, cache=F>>=
  library(car)
  library(leaps)
  library(corrplot)
  baseball <- read.csv("../../data/baseball.csv", na.strings="")
  challenge <- read.csv("../../data/challenge.csv", na.strings="")
  par(fg='#fefefe', col.axis='#fefefe', col.lab='#fefefe', col.main="#fefefe", mar=c(5.1, 4.1, 1.1, 2.1))
  @

  \frame{\maketitle}

  % Show outline at beginning of each section
  \AtBeginSection[]{ 
    \begin{frame}<beamer>
      \tableofcontents[currentsection]
    \end{frame}
  }

  %%%%%%% Slides start here %%%%%%%

  \begin{darkframes}
    
    %{slide 1}
    \begin{frame}{Let's model the batting averages of baseball players}
      \begin{columns}[onlytextwidth]
        \column{.4\textwidth}
          \includegraphics[width=1.6in]{TyCobb.png}
        
        \column{.6\textwidth}
          \begin{itemize}
            \item All of the data here came from \url{http://seanlahman.com/baseball-archive/statistics/}
            \item Some data cleaning was done, mostly to calculate averages
            \item We are going to explore this dataset with best subsets regression
          \end{itemize}
      \end{columns}
    \end{frame}

    \begin{frame}{The response variable}
      \begin{itemize}
        \item \textbf{AVG}: Batting average
      \end{itemize}
    \end{frame}

    %{slide 3}
    \begin{frame}[fragile]{The potential predictors}
      \fontsize{10}{10}\selectfont
      \begin{itemize}
        \item \textbf{YEAR}:         Year this entry calculated for 
        \item \textbf{LG}:           League, either NL or AL
        \item \textbf{OBP}:          On base percentage
        \item \textbf{SLG}:          Slugging average
        \item \textbf{EXP}:          Years of experience
        \item \textbf{PAYR}:         Plate appearances per year
        \item \textbf{MLAVG}:        Batting average for the leauge for the year
        \item \textbf{MLOBP}:        On base percentage for the leaugue for the year
        \item \textbf{MLSLG}:        Slugging percentage for the leaugue for the year
        \item \textbf{AVGcumLag1}:   Player's cumulative batting average for previous years
        \item \textbf{OBPcumLag1}:   Player's cumulative on base percentage for previous years
        \item \textbf{SLGcumLag1}:   Player's cumulative slugging percentage for previous years
        \item \textbf{G}:            Games played (must have been at least 98)
        \item \textbf{YRINDEX}:      Number of years since 1958
      \end{itemize}

      \lc %{Why predict batting average instead of hits?}
    \end{frame}


    %{slide 4}
    \begin{frame}[fragile]{Build model full and check for multicollinearity}
      \fontsize{9}{9}\selectfont
      <<echo=F>>=
      options(digits=3)
      @
      <<>>=
      full <- lm(AVG ~ OBP + SLG + EXP + PAYR + MLAVG 
                        + MLOBP + MLSLG + AVGcumLag1 + OBPcumLag1 
                        + SLGcumLag1 + G + YRINDEX, data=baseball)

      vif(full)
      @
      
      \pause
      \fontsize{10}{10}\selectfont
      Uh oh. Houston, we have a problem!

      \lc %{What is the R2 of model full?}
    \end{frame}


    %{slide #5}
    \begin{frame}[fragile]{Look at the correlations to find the problem}
      Columns 8-19 in the data set are numeric. Let's pull those out and look at the correlation matrix.

      <<results='hide'>>=
      numeric.predictors <- baseball[,8:19]
      cor(numeric.predictors)
      @
      
      \lc %{What is the correlation between OBP and SLG?}
    \end{frame}


    %{slide #6}
    \begin{frame}[fragile]{A correlation plot is easier to read!}
      \fontsize{9}{9}\selectfont 
      <<fig.height=2.4>>=
      library(corrplot)
      corrplot(cor(numeric.predictors))
      @
    \end{frame}

    %{slide 7}
    \begin{frame}[fragile]{Reduce multicollinearity by dropping variables}
      The Major League averages are highly correlated with each other; 
      let's keep just MLAVG and drop MLOBP and MLSLG. (This choice depends on our preference of which
      variable would make the most sense to keep.)

      \fontsize{9}{9}\selectfont
      <<>>=
      full <- lm(AVG ~ OBP + SLG + EXP + PAYR + MLAVG 
                        + AVGcumLag1 + OBPcumLag1 
                        + SLGcumLag1 + G + YRINDEX, data=baseball)
      
      vif(full)
      @

      {Much better!}
    \end{frame}


    %{slide 8}
    \begin{frame}[fragile]{Use best-subsets regression to get a sense of the best predictors}
      \fontsize{9}{9}\selectfont 
      <<>>=
      <<fig.keep='none'>>=
      bestsubsets <- regsubsets(AVG ~ OBP + SLG + EXP + PAYR + MLAVG 
        + AVGcumLag1 + OBPcumLag1 + SLGcumLag1 + G + YRINDEX, data=baseball)
      plot(bestsubsets, scale="adjr2")
      @

      \vspace{-1in}

      <<echo=F, fig.height=3.5>>=
      plot(bestsubsets, scale="adjr2", col='orange')
      @
    \end{frame}


    %{slide 9}
    \begin{frame}[fragile]{Use best-subsets regression to get a sense of the best predictors}
      \fontsize{9}{9}\selectfont
      <<fig.keep='none'>>=
      plot(bestsubsets, scale="bic")
      @

      \vspace{-1in}

      <<echo=F, fig.height=3.5>>=
      plot(bestsubsets, scale="bic", col='orange')
      @
      \lc %{What are the best predictors}
    \end{frame}     


    %{slide 11}
    \begin{frame}[fragile]{Generate the best candidate model}
      \fontsize{8}{8}\selectfont 
      <<echo=F>>=
      model <- lm(AVG ~ OBP + SLG + AVGcumLag1 + OBPcumLag1 + SLGcumLag1, data=baseball)
      summary(model)
      @
    \end{frame}

    %{slide 13}
    \begin{frame}[fragile]{Does the National League's Designated Hitter Rule Matter?}
      Let's first look at only the cases where LG is either NL or AL, to simplify the analysis (other rows correspond to a player that switched teams during the season). Then we'll add LG to the model.

      <<>>=
      base1 <- baseball[baseball$LG == "NL" | 
                        baseball$LG == "AL",]
      modelLG <- lm(AVG ~ OBP + SLG + AVGcumLag1 + OBPcumLag1
                            + SLGcumLag1 + LG, data=base1)
      @
    \end{frame}

    \begin{frame}[fragile]
      \fontsize{8}{8}\selectfont
      <<>>=
      summary(modelLG)
      @
      \lc %{What is the p-value of league?}
      \lc %{Does the designated hitter rule predict}
    \end{frame}

    %{slide 15}
    \begin{frame}[fragile]{Is this model really useful?}
      \begin{itemize}[<+->]
        \item Automated regression model selection methods cannot make something out of nothing. 
        \item If you omit some important variables or fail to use data transformations when they are needed, or if the assumption of linear or linearizable relationships is simply wrong, the model is a bad one, no matter what the $R^2$.  
        \item Use your own judgment and intuition about your data to try to fine-tune whatever the computer comes up with.
      \end{itemize} 
    \end{frame}

    \begin{frame}[fragile]{Surprise!}
      This data is all random numbers! Here's how it was generated:

      <<>>=
      y <- rnorm(100)
      x1 <- rnorm(100)
      x2 <- rnorm(100)
      # etc
      @

      $R^2=0.21$, so 21\% of the variance in $Y$ is explained by random numbers!  
      \lc %{What did I do wrong?}
    \end{frame}

    \begin{frame}[fragile]{Be careful of spurious correlations and overfitting!}
      \begin{itemize}
        \item If you have more than 1 predictor for 10-15 cases, you are likely to see spurious correlations.
        \item If you fit models with meaningless variables, you are fitting noise and will end up with an \alert{overfit} model that is not predictive on new data. 
      \end{itemize} 
    \end{frame}

    \begin{frame}
      \fullpagepicture{spurious-correlation}
    \end{frame}
  \end{darkframes}
\end{document}