\documentclass{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{1, 0.894, 0.769}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.824,0.412,0.118}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{1,0.894,0.71}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.824,0.706,0.549}{#1}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.941,0.902,0.549}{#1}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.804,0.776,0.451}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.78,0.941,0.545}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{1,0.78,0.769}{#1}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{../371g-slides}
\title{Logistic Regression 1}
\subtitle{Lecture 16}
\author{STA 371G}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
  
  

  \frame{\maketitle}

  % Show outline at beginning of each section
  \AtBeginSection[]{ 
    \begin{frame}<beamer>
      \tableofcontents[currentsection]
    \end{frame}
  }

  %%%%%%% Slides start here %%%%%%%

  \begin{darkframes}
    \begin{frame}{Near, far, wherever you are....}
      \begin{itemize}
        \item How much did the ticket price affect whether someone survived the Titanic?
        \item We have a data set of 1045 passengers on the Titanic; 427 survived
          \begin{itemize}
            \item \textbf{fare}: the amount of money paid for the ticket
            \item \textbf{survived}: a dummy variable indicating whether the passenger surived (1) or not (0)
          \end{itemize}
        \pause
        \item What is unusual about this data set?
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{What goes wrong with linear regression?}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(titanic}\hlopt{$}\hlstd{fare, titanic}\hlopt{$}\hlstd{survived)}
\hlstd{model} \hlkwb{<-} \hlkwd{lm}\hlstd{(survived} \hlopt{~} \hlstd{fare,} \hlkwc{data}\hlstd{=titanic)}
\hlkwd{abline}\hlstd{(model)}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-2-1.tikz}

\end{knitrout}
    \end{frame}

    \begin{frame}{The idea behind logistic regression}
      \begin{itemize}
        \item Instead of predicting whether someone survives, let's predict the \emph{probability} that they survice
        \item Let's fit a curve that is always between 0 and 1
      \end{itemize}
    \end{frame}

    \begin{frame}{Odds}
      \begin{itemize}
        \item When something has ``even (1/1) odds,'' the probability of success is $1/2$
        \item When something has ``2/1 odds,'' the probability of success is $2/3$
        \item When something has ``3/2 odds,'' the probability of success is $3/5$
        \item In general, the odds of something happening are $p/(1-p)$
      \end{itemize}
      \lc
    \end{frame}

    \begin{frame}{The logistic regression model}
      Logistic regression models the \alert{log odds} of success $p$ as a linear function of $X$:
      \[
        \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X + \epsilon
      \]
      This fits an S-shaped curve to the data (we'll see what it looks like later).
    \end{frame}

    \begin{frame}[fragile]{Let's try it}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{model} \hlkwb{<-} \hlkwd{glm}\hlstd{(survived} \hlopt{~} \hlstd{fare,} \hlkwc{data}\hlstd{=titanic,}
             \hlkwc{family}\hlstd{=binomial)}
\hlkwd{summary}\hlstd{(model)}
\end{alltt}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}{How to interpret the curve?}
      
      The regression output tells us that our prediction is
      \[
        \log\left(\frac{P(\text{survival})}{1-P(\text{survival})}\right) = -0.794 + 0.012\cdot\text{fare}.
      \]
      \pause
      Let's solve for $P(\text{survival})$:
      \[
        \widehat{P(\text{survival})} = \frac{e^{-0.794 + 0.012\cdot\text{fare}}}{1 + e^{-0.794 + 0.012\cdot\text{fare}}}
      \]
      \lc
    \end{frame}

    \begin{frame}[fragile]{Making predictions}
      We can use \texttt{predict} to automate the process of plugging into the equation:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlkwd{predict}\hlstd{(model,} \hlkwd{data.frame}\hlstd{(}\hlkwc{fare}\hlstd{=}\hlnum{50}\hlstd{),} \hlkwc{type}\hlstd{=}\hlstr{"response"}\hlstd{)}
\end{alltt}
\begin{verbatim}
    1 
0.453 
\end{verbatim}
\end{kframe}
\end{knitrout}
      
      \[
        \frac{e^{-0.794 + 0.012\cdot 50}}{1 + e^{-0.794 + 0.012\cdot 50}} = 0.453
      \]
    \end{frame}

    \begin{frame}[fragile]{How to interpret the curve?}
      \[
        \widehat{P(\text{survival})} = \frac{e^{-0.794 + 0.012\cdot\text{fare}}}{1 + e^{-0.794 + 0.012\cdot\text{fare}}}
      \]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}
\input{/tmp/figures/unnamed-chunk-7-1.tikz}

\end{knitrout}
    \end{frame}

    \begin{frame}{Interpreting the coefficients}
      Our prediction equation is:
      \[
        \log\left(\frac{P(\text{survival})}{1-P(\text{survival})}\right) = -0.794 + 0.012\cdot\text{fare}.
      \]
      Let's start with some basic, but not particularly useful, interpretations:
      \begin{itemize}[<+->]
        \item When $\text{fare}=0$, we predict that the log odds will be $-0.794$ \pause, so the probability or survival is predicted to be $0.29$.
        \item When fare increases by \pounds 1, we predict that the log odds will decrease by $0.012$.
      \end{itemize}
      \note{LC question in the middle}
    \end{frame}

    \begin{frame}{Interpreting the coefficients}
      Let's rewrite the prediction equation as:
      \[
        \text{Predicted odds of survival} = e^{-0.794 + 0.012\cdot\text{fare}}
      \]
      Increasing the fare by \pounds 1 will \emph{multiply} the odds by $e^{0.012}=1.012$; i.e., increase the odds by $1.2$\%.

      \bigskip\pause
      Increasing the fare by \pounds 10 will \emph{multiply} the odds by $e^{10 \cdot 0.012}=1.129$; i.e., increase the odds by $12.9$\%.
    \end{frame}

    \begin{frame}{Testing the null hypothesis}
      \begin{center}
        As in regular linear regression, the overall null hypothesis is that $\beta_1=0$; we can test this by using the $p$-value for that variable on the output.

        \bigskip\pause
        Since $p$ is very small, we can reject the null hypothesis that $\beta_1=0$; i.e., there is a statistically significant relationship between fare and survival.
      \end{center}
    \end{frame}

    \begin{frame}{How good is our model?}
      \begin{itemize}[<+->]
        \item Unfortunately, the typical $R^2$ metric isn't available for logistic regression.
        \item However, there are many ``pseudo-$R^2$'' metrics that indicate model fit.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Take 1: How many cases did we accurately predict?}
      We could use our model to make a prediction of survival (or not), based on the probability. Suppose we say that our prediction is:
      \[
        \text{Prediction} = \begin{cases}
          \text{survival}, & \text{if $\widehat{P(\text{survival})} \geq 0.5$}, \\
          \text{tragedy}, & \text{if $\widehat{P(\text{survival})} < 0.5$}. \\
        \end{cases}
      \]
      
      \pause
      Now we can compute the fraction of people whose survival we correctly predicted:

      \fontsize{9}{9}\selectfont
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{predicted.survival} \hlkwb{<-} \hlstd{(}\hlkwd{predict}\hlstd{(model,} \hlkwc{type}\hlstd{=}\hlstr{'response'}\hlstd{)} \hlopt{>=} \hlnum{0.5}\hlstd{)}
\hlstd{actual.survival} \hlkwb{<-} \hlstd{(titanic}\hlopt{$}\hlstd{survived} \hlopt{==} \hlnum{1}\hlstd{)}
\hlkwd{sum}\hlstd{(predicted.survival} \hlopt{==} \hlstd{actual.survival)} \hlopt{/} \hlkwd{nrow}\hlstd{(titanic)}
\end{alltt}
\begin{verbatim}
[1] 0.644
\end{verbatim}
\end{kframe}
\end{knitrout}
      \note{LC question in middle}
    \end{frame}

    \begin{frame}[fragile]{Take 1: How many cases did we accurately predict?}
      64.4\% sounds pretty good---what should we compare it against?

      \bigskip
      \pause

      We should compare 64.4\% against what we would have gotten if we just predicted the most common outcome (not surviving) for everyone, without using any other information:
      \pause
      
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlnum{1} \hlopt{-} \hlkwd{sum}\hlstd{(actual.survival)} \hlopt{/} \hlkwd{nrow}\hlstd{(titanic)}
\end{alltt}
\begin{verbatim}
[1] 0.591
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}{Take 2: McFadden's pseudo-$R^2$}
      To get a metric on the usual 0-1 scale (like regular $R^2$), McFadden's pseudo-$R^2$ can be used (this is what is described in the reading):
      \[
        \text{pseudo-}R^2 = 1 - \frac{\text{residual deviance}}{\text{null deviance}} = 1 - \frac{1339.941}{1413.571} = 0.052
      \]
    \end{frame}

    \begin{frame}[fragile]
      \fontsize{9}{9}\selectfont
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{verbatim}

Call:
glm(formula = survived ~ fare, family = binomial, data = titanic)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-2.231  -0.928  -0.898   1.335   1.528  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -0.7941     0.0844   -9.41  < 2e-16 ***
fare          0.0121     0.0017    7.13  9.9e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1413.6  on 1044  degrees of freedom
Residual deviance: 1339.9  on 1043  degrees of freedom
AIC: 1344

Number of Fisher Scoring iterations: 4
\end{verbatim}
\end{kframe}
\end{knitrout}
      \note{Do extra LC practice questions if time.}
    \end{frame}

    \begin{frame}{What else can we use logistic regression for?}
      \begin{itemize}
        \item \textbf{Finance:} Predicting which customers are most likely to default on a loan
        \item \textbf{Advertising:} Predicting when a customer will respond positively to an advertising campaign
        \item \textbf{Marketing:} Predicting when a customer will purchase a product or sign up for a service
      \end{itemize}
    \end{frame}
  \end{darkframes}

\end{document}
